setwd('C:\\Users\\vitor\\Google Drive\\DS\\R-DataVisualization\\3')
dir.create("C:\\Users\\vitor\\Google Drive\\DS\\R-DataVisualization\\3\\data")
download.file("https://ibm.box.com/shared/static/cmid70rpa7xe4ocitcga1bve7r0kqnia.txt",
destfile = "C:\\Users\\vitor\\Google Drive\\DS\\R-DataVisualization\\3\\data\\churchill_speeches.txt", quiet = TRUE)
library(tm)
library(wordcloud)
dirPath = 'C:\\Users\\vitor\\Google Drive\\DS\\R-DataVisualization\\3\\data'
speech = Corpus(DirSource(dirPath))
inspect(speech)
#Data Cleaning:
#Convert to lower case:
speech = tm_map(speech, content_transformer(tolower))
speech = tm_map(speech, removeNumbers)
#Remove stopwords from the languague english
speech = tm_map(speech, removeWords,
stopwords("english"))
speech = tm_map(speech, removePunctuation)
speech = tm_map(speech, stripWhitespace)
#Create a Term Document Matrix
#Document which contains the frequency of the words:
dtm = TermDocumentMatrix(speech)
m = as.matrix(dtm)
#Sort the words by frequency from highets to lowest:
v = sort(rowSums(m), decreasing = T)
d = data.frame(word = names(v), freq = v)
View(dtm)
View(d)
head(d,10)
#Creating the wordcloud:
#Only the main ones:
wordcloud(words = d$word, freq = d$freq)
#Showing every word: (definying the minimun frequency)
wordcloud(words = d$word, freq = d$freq,
min.freq = 1, max.words = 250,
shape = 'circle')
#Creating the wordcloud:
#Only the main ones:
wordcloud(words = d$word, freq = d$freq)
#Showing every word: (definying the minimun frequency)
wordcloud(words = d$word, freq = d$freq,
min.freq = 2, max.words = 250,
shape = 'circle')
#Showing every word: (definying the minimun frequency)
wordcloud(words = d$word, freq = d$freq,
min.freq = 2, max.words = 250,
shape = 'circle', colors = brewer.pal(8, "Dark2"),
random.order = F)
